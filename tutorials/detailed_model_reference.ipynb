{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e446da2d668c2e07",
   "metadata": {},
   "source": [
    "# LaB-GATr Detailed Model Reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76af12cabee4c7e",
   "metadata": {},
   "source": [
    "## 0. Installation\n",
    "\n",
    "In this notebook we explain the inner blocks of LaB-GATr.\n",
    "\n",
    "Before using this notebook, install the correct dependencies and LaB-GATr as follows:\n",
    "\n",
    "Optional new Anaconda environment\n",
    "```\n",
    "conda create --name lab-gatr python=3.10\n",
    "conda activate lab-gatr\n",
    "``` \n",
    "Next, install PyTorch and xFormers and other libraries\n",
    "```\n",
    "pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install xformers==0.0.22.post7 --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install torch_geometric==2.4.0\n",
    "pip install torch_scatter torch_cluster -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "```\n",
    "Install LaB-GATr itself, which also install GATr\n",
    "```\n",
    "pip install .\n",
    "```\n",
    "Additionally, if you have made a new Anaconda environment, install Jupyter\n",
    "```\n",
    "pip install jupyter jupyterlab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e38155c4ab6cc4",
   "metadata": {},
   "source": [
    "## 1. Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:26.872334Z",
     "start_time": "2024-07-01T09:18:26.869500Z"
    }
   },
   "source": [
    "from gatr.interface import embed_oriented_plane, extract_translation\n",
    "import torch\n",
    "import gatr\n",
    "from lab_gatr.nn.class_token import class_token_forward_wrapper\n",
    "from xformers.ops.fmha import BlockDiagonalMask\n",
    "\n",
    "from lab_gatr.nn.mlp.geometric_algebra import MLP\n",
    "from lab_gatr.nn.gnn import PointCloudPooling, pool\n",
    "from torch_scatter import scatter\n",
    "from gatr.interface import embed_translation\n",
    "\n",
    "import torch_geometric as pyg\n",
    "from lab_gatr.data import Data\n",
    "from torch_cluster import fps, knn"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "89de2d017dd9efa3",
   "metadata": {},
   "source": [
    "Let us first create a dummy mesh: n positions and orientations (e.g. surface normal) and an arbitrary scalar feature (e.g. geodesic distance)."
   ]
  },
  {
   "cell_type": "code",
   "id": "7264db0cc4bfa961",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:26.925259Z",
     "start_time": "2024-07-01T09:18:26.922672Z"
    }
   },
   "source": [
    "n = 1000\n",
    "\n",
    "pos, orientation = torch.rand((n, 3)), torch.rand((n, 3))\n",
    "scalar_feature = torch.rand(n)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "cb6a1fa784bd6801",
   "metadata": {},
   "source": [
    "Next, a point cloud pooling transform for the tokenisation (patching).\n",
    "\n",
    "Nested hierarchy of sub-sampled point clouds. Each coarse-scale point is mapped to a cluster of fine-scale points. Barycentric interpolation from the coarse to the fine scales. For correct batching, ```torch_geometric.data.Data.__inc__()``` has to be overridden.\n",
    "\n",
    "```TODO```"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac963dbde04fc5c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:26.931778Z",
     "start_time": "2024-07-01T09:18:26.926656Z"
    }
   },
   "source": [
    "class PointCloudPoolingScales():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        rel_sampling_ratios (tuple): relative ratios for successive farthest point sampling\n",
    "        interp_simplex (str): reference simplex for barycentric interpolation ('triangle' or 'tetrahedron')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rel_sampling_ratios: tuple, interp_simplex: str):\n",
    "        self.rel_sampling_ratios = rel_sampling_ratios\n",
    "        self.interp_simplex = interp_simplex\n",
    "\n",
    "        self.dim_interp_simplex = {'triangle': 2, 'tetrahedron': 3}[interp_simplex]\n",
    "\n",
    "    def __call__(self, data: pyg.data.Data) -> Data:\n",
    "\n",
    "        pos = data.pos\n",
    "        batch = data.surface_id.long() if hasattr(data, 'surface_id') else torch.zeros(pos.size(0), dtype=torch.long)\n",
    "\n",
    "        for i, sampling_ratio in enumerate(self.rel_sampling_ratios):\n",
    "\n",
    "            sampling_idcs = fps(pos, batch, ratio=sampling_ratio)  # takes some time but is worth it\n",
    "            # sampling_idcs = torch.arange(0, pos.size(0), 1. / sampling_ratio, dtype=torch.int)\n",
    "\n",
    "            pool_source, pool_target = knn(pos[sampling_idcs], pos, 1, batch[sampling_idcs], batch)\n",
    "            interp_target, interp_source = knn(pos[sampling_idcs], pos, self.dim_interp_simplex + 1, batch[sampling_idcs], batch)\n",
    "\n",
    "            data[f'scale{i}_pool_target'], data[f'scale{i}_pool_source'] = pool_target.int(), pool_source.int()\n",
    "            data[f'scale{i}_interp_target'], data[f'scale{i}_interp_source'] = interp_target.int(), interp_source.int()\n",
    "            data[f'scale{i}_sampling_index'] = sampling_idcs.int()\n",
    "\n",
    "            pos = pos[sampling_idcs]\n",
    "            batch = batch[sampling_idcs]\n",
    "\n",
    "        return Data(**data)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(rel_sampling_ratios={self.rel_sampling_ratios}, interp_simplex={self.interp_simplex})\""
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8b415d91655e9c83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "transform = PointCloudPoolingScales(rel_sampling_ratios=(0.2,), interp_simplex='triangle')\n",
    "dummy_data = transform(pyg.data.Data(pos=pos, orientation=orientation, scalar_feature=scalar_feature))"
   ],
   "id": "54400f8d7cb85dd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Building the Model",
   "id": "15893f75d230b52e"
  },
  {
   "cell_type": "markdown",
   "id": "546e0c748430850a",
   "metadata": {},
   "source": [
    "A geometric algebra interface to embed your data in $\\mathbf{G}(3, 0, 1)$.\n",
    "\n",
    "```TODO```"
   ]
  },
  {
   "cell_type": "code",
   "id": "375b5f1d6f0c856b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:26.949093Z",
     "start_time": "2024-07-01T09:18:26.945733Z"
    }
   },
   "source": [
    "class GeometricAlgebraInterface:\n",
    "    num_input_channels = num_output_channels = 1\n",
    "    num_input_scalars = num_output_scalars = 1\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def embed(data):\n",
    "\n",
    "        multivectors = embed_oriented_plane(normal=data.orientation, position=data.pos).view(-1, 1, 16)\n",
    "        scalars = data.scalar_feature.view(-1, 1)\n",
    "\n",
    "        return multivectors, scalars\n",
    "\n",
    "    @staticmethod\n",
    "    def dislodge(multivectors, scalars):\n",
    "        return extract_translation(multivectors).squeeze()\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "c832f5754fbd5911",
   "metadata": {},
   "source": [
    "### 2.1 Tokenizer\n",
    "```TODO```"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:26.954579Z",
     "start_time": "2024-07-01T09:18:26.950209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def interp(\n",
    "    mlp: torch.nn.Module,\n",
    "    multivectors: torch.Tensor,\n",
    "    multivectors_skip: torch.Tensor,\n",
    "    scalars: torch.Tensor,\n",
    "    scalars_skip: torch.Tensor,\n",
    "    pos_source: torch.Tensor,\n",
    "    pos_target: torch.Tensor,\n",
    "    data: Data,\n",
    "    scale_id: int,\n",
    "    reference_multivector: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    pos_diff = pos_source[data[f'scale{scale_id}_interp_source']] - pos_target[data[f'scale{scale_id}_interp_target']]\n",
    "    squared_pos_dist = torch.clamp(torch.sum(pos_diff ** 2, dim=-1), min=1e-16).view(-1, 1, 1)\n",
    "\n",
    "    denominator = scatter(1. / squared_pos_dist, data[f'scale{scale_id}_interp_target'].long(), dim=0, reduce='sum')\n",
    "\n",
    "    multivectors = scatter(\n",
    "        multivectors[data[f'scale{scale_id}_interp_source']] / squared_pos_dist,\n",
    "        data[f'scale{scale_id}_interp_target'].long(),\n",
    "        dim=0,\n",
    "        reduce='sum'\n",
    "    ) / denominator\n",
    "\n",
    "    scalars = scatter(\n",
    "        scalars[data[f'scale{scale_id}_interp_source']] / squared_pos_dist.view(-1, 1),\n",
    "        data[f'scale{scale_id}_interp_target'].long(),\n",
    "        dim=0,\n",
    "        reduce='sum'\n",
    "    ) / denominator.view(-1, 1)\n",
    "\n",
    "    multivectors = torch.cat((multivectors, multivectors_skip), dim=-2)\n",
    "    scalars = torch.cat((scalars, scalars_skip), dim=-1)\n",
    "\n",
    "    return mlp(multivectors, scalars, reference_mv=reference_multivector)"
   ],
   "id": "692225270547a492",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "```TODO```",
   "id": "86c7c79f3c53ead6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:26.959591Z",
     "start_time": "2024-07-01T09:18:26.955915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PointCloudPooling(PointCloudPooling):\n",
    "\n",
    "    def message(\n",
    "        self,\n",
    "        x_j: torch.Tensor,\n",
    "        pos_i: torch.Tensor,\n",
    "        pos_j: torch.Tensor,\n",
    "        scalars_j: torch.Tensor,\n",
    "        reference_multivector_j: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        multivectors, scalars = self.mlp(\n",
    "            torch.cat((x_j, embed_translation(pos_j - pos_i).unsqueeze(-2)), dim=-2),\n",
    "            scalars=scalars_j,\n",
    "            reference_mv=reference_multivector_j\n",
    "        )\n",
    "\n",
    "        return multivectors, scalars\n",
    "\n",
    "    def aggregate(self, inputs: tuple, index: torch.Tensor, ptr=None, dim_size=None) -> torch.Tensor:\n",
    "        multivectors, scalars = (self.aggr_module(tensor, index, ptr=ptr, dim_size=dim_size, dim=self.node_dim) for tensor in inputs)\n",
    "\n",
    "        return multivectors, scalars"
   ],
   "id": "dc8b7295bcc0eebb",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "```TODO```",
   "id": "c87478bffc122b63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:26.985563Z",
     "start_time": "2024-07-01T09:18:26.973937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tokeniser(torch.nn.Module):\n",
    "    def __init__(self, geometric_algebra_interface: object, d_model: int, num_latent_channels=None, dropout_probability=None):\n",
    "        super().__init__()\n",
    "        self.geometric_algebra_interface = geometric_algebra_interface()\n",
    "\n",
    "        num_input_channels = self.geometric_algebra_interface.num_input_channels\n",
    "        num_output_channels = self.geometric_algebra_interface.num_output_channels\n",
    "\n",
    "        num_input_scalars = self.geometric_algebra_interface.num_input_scalars\n",
    "        num_output_scalars = self.geometric_algebra_interface.num_output_scalars\n",
    "\n",
    "        num_latent_channels = num_latent_channels or d_model\n",
    "\n",
    "        self.point_cloud_pooling = PointCloudPooling(MLP(\n",
    "            (num_input_channels + 1, num_latent_channels, d_model),\n",
    "            num_input_scalars,\n",
    "            num_output_scalars=num_input_scalars,\n",
    "            plain_last=False,\n",
    "            use_norm_in_first=False,\n",
    "            dropout_probability=dropout_probability\n",
    "        ), node_dim=0)\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            (d_model + num_input_channels, *[num_latent_channels] * 2, num_output_channels),\n",
    "            num_input_scalars=2 * num_input_scalars,\n",
    "            num_output_scalars=num_output_scalars,\n",
    "            use_norm_in_first=False,\n",
    "            dropout_probability=dropout_probability\n",
    "        )\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        multivectors, scalars = self.geometric_algebra_interface.embed(data)\n",
    "\n",
    "        self.cache = {\n",
    "            'multivectors': multivectors,\n",
    "            'scalars': scalars,\n",
    "            'data': data,\n",
    "            'reference_multivector': self.construct_reference_multivector(multivectors, data.batch)\n",
    "        }\n",
    "\n",
    "        (multivectors, scalars), self.cache['pos'] = pool(\n",
    "            self.point_cloud_pooling,\n",
    "            multivectors,\n",
    "            data.pos,\n",
    "            data,\n",
    "            scale_id=0,\n",
    "            scalars=scalars,\n",
    "            reference_multivector=self.cache['reference_multivector']\n",
    "        )\n",
    "\n",
    "        return multivectors, scalars, self.cache['reference_multivector'][data.scale0_sampling_index]\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_reference_multivector(x: torch.Tensor, batch=None):\n",
    "\n",
    "        if batch is None:\n",
    "            reference_multivector = x.mean(dim=(0,1)).expand(x.size(0), 1, -1)\n",
    "\n",
    "        else:\n",
    "            reference_multivector = scatter(x, batch, dim=0, reduce='mean').mean(dim=1, keepdim=True)[batch]\n",
    "\n",
    "        return reference_multivector\n",
    "\n",
    "    def lift(self, multivectors: torch.Tensor, scalars: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        if multivectors.size(0) == self.cache['data'].scale0_sampling_index.numel():\n",
    "            multivectors, scalars = interp(\n",
    "                self.mlp,\n",
    "                multivectors,\n",
    "                self.cache['multivectors'],\n",
    "                scalars,\n",
    "                self.cache['scalars'],\n",
    "                self.cache['pos'],\n",
    "                self.cache['data'].pos,\n",
    "                self.cache['data'],\n",
    "                scale_id=0,\n",
    "                reference_multivector=self.cache['reference_multivector']\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            multivectors, scalars = self.extract_class(\n",
    "                self.mlp,\n",
    "                multivectors,\n",
    "                self.cache['multivectors'],\n",
    "                scalars,\n",
    "                self.cache['scalars'],\n",
    "                self.cache['data']\n",
    "            )\n",
    "\n",
    "        return self.geometric_algebra_interface.dislodge(multivectors, scalars)\n",
    "\n",
    "    def extract_class(\n",
    "        self,\n",
    "        mlp: MLP,\n",
    "        multivectors: torch.Tensor,\n",
    "        multivectors_skip: torch.Tensor,\n",
    "        scalars: torch.Tensor,\n",
    "        scalars_skip: torch.Tensor,\n",
    "        data: Data\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        if data.batch is None:\n",
    "            multivectors_skip = multivectors_skip.mean(dim=0, keepdim=True)\n",
    "            scalars_skip = scalars_skip.mean(dim=0, keepdim=True)\n",
    "\n",
    "            reference_multivector = self.cache['reference_multivector'][0:1]\n",
    "\n",
    "        else:\n",
    "            multivectors_skip = scatter(multivectors_skip, data.batch, dim=0, reduce='mean')\n",
    "            scalars_skip = scatter(scalars_skip, data.batch, dim=0, reduce='mean')\n",
    "\n",
    "            reference_multivector = self.cache['reference_multivector'][data.ptr[:-1]]\n",
    "\n",
    "        multivectors = torch.cat((multivectors, multivectors_skip), dim=-2)\n",
    "        scalars = torch.cat((scalars, scalars_skip), dim=-1)\n",
    "\n",
    "        return mlp(multivectors, scalars, reference_mv=reference_multivector)\n",
    "\n"
   ],
   "id": "c2083f9c4b8a1833",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 LaB-GATr",
   "id": "d8a241a8fdd731e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "```TODO```",
   "id": "ff4c4923dd10a3d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:26.993182Z",
     "start_time": "2024-07-01T09:18:26.987283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LaBGATr(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        geometric_algebra_interface: object,\n",
    "        d_model: int,\n",
    "        num_blocks: int,\n",
    "        num_attn_heads: int,\n",
    "        num_latent_channels=None,\n",
    "        use_class_token: bool = False,\n",
    "        dropout_probability=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        num_latent_channels = num_latent_channels or d_model\n",
    "\n",
    "        self.tokeniser = Tokeniser(\n",
    "            geometric_algebra_interface,\n",
    "            d_model,\n",
    "            num_latent_channels=4 * num_latent_channels,\n",
    "            dropout_probability=dropout_probability\n",
    "        )\n",
    "\n",
    "        self.gatr = gatr.GATr(\n",
    "            in_mv_channels=d_model,\n",
    "            out_mv_channels=d_model,\n",
    "            hidden_mv_channels=num_latent_channels,\n",
    "            in_s_channels=geometric_algebra_interface.num_input_scalars,\n",
    "            out_s_channels=geometric_algebra_interface.num_input_scalars,\n",
    "            hidden_s_channels=4 * num_latent_channels,\n",
    "            attention=gatr.SelfAttentionConfig(num_heads=num_attn_heads),\n",
    "            mlp=gatr.MLPConfig(),\n",
    "            num_blocks=num_blocks,\n",
    "            dropout_prob=dropout_probability\n",
    "        )\n",
    "\n",
    "        if use_class_token:\n",
    "            self.gatr.forward = class_token_forward_wrapper(self.gatr.forward)\n",
    "\n",
    "        self.num_parameters = sum(parameter.numel() for parameter in self.parameters() if parameter.requires_grad)\n",
    "        print(f\"LaB-GATr ({self.num_parameters} parameters)\")\n",
    "\n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        multivectors, scalars, reference_multivector = self.tokeniser(data)\n",
    "\n",
    "        multivectors, scalars = self.gatr(\n",
    "            multivectors,\n",
    "            scalars=scalars,\n",
    "            attention_mask=self.get_attn_mask(data),\n",
    "            join_reference=reference_multivector\n",
    "        )\n",
    "\n",
    "        return self.tokeniser.lift(multivectors, scalars)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_attn_mask(data: Data):\n",
    "\n",
    "        if data.batch is None:\n",
    "            attn_mask = None\n",
    "\n",
    "        else:\n",
    "            batch = data.batch[data.scale0_sampling_index]\n",
    "            attn_mask = BlockDiagonalMask.from_seqlens(torch.bincount(batch).tolist())\n",
    "\n",
    "        return attn_mask"
   ],
   "id": "65517f52821b5ea4",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Usage",
   "id": "34fe726c4051ee23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:27.087550Z",
     "start_time": "2024-07-01T09:18:26.994939Z"
    }
   },
   "cell_type": "code",
   "source": "model = LaBGATr(GeometricAlgebraInterface, d_model=8, num_blocks=10, num_attn_heads=4, use_class_token=False)",
   "id": "63280835758baa8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaB-GATr (317180 parameters)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "38764051e0c48259",
   "metadata": {},
   "source": [
    "Generate some output with the dummy data to verify that the model functions. Training or inference from here on is the same as any PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "id": "4dfd82278687c9c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:18:27.222515Z",
     "start_time": "2024-07-01T09:18:27.088349Z"
    }
   },
   "source": [
    "output = model(dummy_data)\n",
    "print(output.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3])\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
